%!TEX root = ../Tibt.tex

\exercise{3.21 and 3.22}

The reduced-rank regression problem can be written as:
\begin{eqnarray} \label{eq:3p21_def}
\hat{B}(m) & = & \argmin_{\textrm{rank}(B) = m} \textrm{Tr} \left[  \left( \mathbf{Y} - \mathbf{X} B \right) \Sigma_Y ^{-1} \left( \mathbf{Y} - \mathbf{X} B \right) ^T \right]
\end{eqnarray}
From now on, we only assume that $\Sigma_Y$ is symmetric and positive-definite, without specifying how it is obtained from the data. We also assume $\Sigma_X \equiv \mathbf{X}^T \mathbf{X}$ to be positive-definite to simplify the discussion. One has:
\begin{eqnarray*}
    \hat{B}(m) & = & \Sigma_X ^{-1/2} \hat{B} ^{\star}(m)\, \Sigma_Y^{1/2} \\
    \hat{B} ^{\star}(m) & = & \argmin_{\textrm{rank}(B ^{\star}) = m} \textrm{Tr} \left[  \left( \mathbf{Y}_r - \mathbf{X}_n B ^{\star} \right) \left( \mathbf{Y}_r - \mathbf{X}_n B ^{\star} \right) ^T \right]\\
    \mathbf{Y}_r & \equiv & \mathbf{Y}\, \Sigma_Y ^{-1/2}\\
    \mathbf{X}_n & \equiv & \mathbf{X}\, \Sigma_X ^{-1/2}: \quad \mathbf{X}_n^T \mathbf{X}_n = \mathbb{I}_p
\end{eqnarray*}
For any $\mathcal{V} \in \mathbb{R}^{p, m}, \; \mathcal{V}^T \mathcal{V} = \mathbb{I}_m$ and $\mathcal{L} \in \mathbb{R}^{m, K}$, the matrix:
\begin{eqnarray} \label{eq:3p21_e0}
&&B ^{\star}  =  \mathcal{V} \mathcal{L}
\end{eqnarray}
has rank $m$. Conversely, any matrix of rank $m$ can be written this way. Indeed, the rank condition is equivalent to the span of the columns of $B ^{\star}$ having dimension $m$, in which case all columns of $B ^{\star}$ can be written as linear combinations of a set of $m$ orthonormal vectors $\mathcal{V}_1, \ldots, \mathcal{V}_m$. Notice that the decomposition (\ref{eq:3p21_e0}) is not unique, since any rotation of the columns of $\mathcal{V}$ leaves $B ^{\star}$ invariant:
\begin{eqnarray}
\mathcal{V} & \rightarrow & \mathcal{V} \mathcal{R}\\
\mathcal{L} & \rightarrow & \mathcal{R}^T \mathcal{L}\\
\mathcal{R} ^T  \mathcal{R} & = &  \mathcal{R}  \mathcal{R} ^T = \mathbb{I}_m
\end{eqnarray}
Keeping this degeneracy in mind, we can write:
\begin{eqnarray*}
    \hat{B} ^{\star}(m) & = & \mathcal{V} ^{\star}(m) \mathcal{L} ^{\star}(m)\\
    \mathcal{V} ^{\star}(m), \; \mathcal{L} ^{\star}(m) & = & \argmin_{\substack{\mathcal{V} ^{\star}, \mathcal{L}^{\star}\\ \mathcal{V}^{\star T} \mathcal{V} ^{\star} = \mathbb{I}_m}} \textrm{Tr} \left[  \left( \mathbf{Y}_r - \mathbf{X}_n \mathcal{V} ^{\star} \mathcal{L} ^{\star} \right) \left( \mathbf{Y}_r - \mathbf{X}_n  \mathcal{V} ^{\star} \mathcal{L} ^{\star} \right) ^T \right]
\end{eqnarray*}
In the absence of constraints, the minimization over $\mathcal{L} ^{\star}$ is a simple OLS procedure, which gives:
\begin{eqnarray*}
    \mathcal{L} ^{\star}(m) = \mathcal{V} ^{\star T}(m) \mathbf{X}_n ^{T} \mathbf{Y}_r
\end{eqnarray*}
One gets:
\begin{eqnarray} \nonumber
\mathcal{V} ^{\star}(m) & = & \argmax_{\mathcal{V} ^{\star}: \; \mathcal{V} ^{\star T} \mathcal{V} ^{\star}\; = \; \mathbb{I}_m} \textrm{Tr} \left( \mathcal{V}^{\star} \mathcal{V} ^{\star T} \mathbf{X}_n^T \mathbf{Y}_r \mathbf{Y}_r^T \mathbf{X}_n \right)\\ \label{eq:3p21_e1}
& = & \argmax_{\mathcal{V} ^{\star}: \; \mathcal{V} ^{\star T} \mathcal{V} ^{\star}\; = \; \mathbb{I}_m} \textrm{Tr} \left( \mathcal{V}^{\star} \mathcal{V} ^{\star T} V ^{\star} D ^{\star 2} V ^{\star T} \right)
\end{eqnarray}
The last equality is obtained by plugging in the the generalized SVD:
\begin{eqnarray} \label{eq:3p21_e2}
\Sigma_Y^{-1/2} (\mathbf{Y}^T \mathbf{X}) \Sigma_X^{-1/2} = \mathbf{Y}_r^T \mathbf{X}_n = U ^{\star} D ^{\star} V ^{\star T}
\end{eqnarray}
The maximization of (\ref{eq:3p21_e1}) can be carried out using the lagrangian:
\begin{eqnarray*}
    \textrm{Tr} \left( \mathcal{V}^{\star} \mathcal{V} ^{\star T} V ^{\star} D ^{\star 2} V ^{\star T} \right) + \textrm{Tr} \left( \Lambda \mathcal{V} ^{\star T} \mathcal{V} \right)
\end{eqnarray*}
The lagrange equations are equivalent to:
\begin{eqnarray*}
    \left( \mathbb{I} - \mathcal{V} ^{\star}(m) \mathcal{V} ^{\star T}(m) \right) V ^{\star} D ^{\star 2} V ^{\star T} \mathcal{V} ^{\star}(m)  = 0
\end{eqnarray*}
This implies that the subspace generated by the columns of $\mathcal{V} ^{\star}(m)$ is stable under the action of the self-adjoint operator $V ^{\star}D ^{\star 2} V ^{\star T}$. This implies that the $m$-dimensional span of the columns of $\mathcal{V} ^{\star}(m)$ is generated my $m$ eigenvectors of $V ^{\star}D ^{\star 2} V ^{\star T}$, i.e. by $m$ columns of $V ^{\star}_{c_1}, \ldots, V ^{\star}_{c_m}$ of $V ^{\star}$. Without loss of generality, we can take $\mathcal{V} ^{\star}(m)$ to be the matrix with columns $V ^{\star}_{c_1}, \ldots, V ^{\star}_{c_m}$. The corresponding value of the maximizand is:
\begin{eqnarray*}
    \textrm{Tr} \Big[ \mathcal{V}^{\star}(m) \mathcal{V} ^{\star T}(m) V ^{\star} D ^{\star 2} V ^{\star T} \Big] = \sum_{i = 1}^{m} D_{c_i, c_i}^2
\end{eqnarray*}
This quantity is clearly maximized by choosing $\{c_i\}_{i=1, \ldots, m} = \left\{ 1, \ldots, m \right\}$. Hence, without loss of generality:
\begin{eqnarray}
\mathcal{V} ^{\star}(m) = V ^{\star}_{(m)}
\end{eqnarray}
Putting everything together:
\begin{eqnarray} \label{eq:3p21_final1}
\hat{B}(m) & = & \Sigma_X ^{-1/2} V ^{\star}_{(m)} V ^{\star T}_{(m)} V ^{\star} D ^{\star} U ^{\star T} \Sigma_Y^{1/2} \\
& = & \Sigma_X ^{-1/2} V ^{\star}_{(m)} D ^{\star}_{(m)} U ^{\star T}_{(m)} \Sigma_Y^{1/2}\\
\label{eq:3p21_final2}
& = & \Sigma_X ^{-1/2} V ^{\star} D ^{\star} U ^{\star T} U ^{\star}_{(m)}U ^{\star T}_{(m)}  \Sigma_Y^{1/2}
\end{eqnarray}
Notice how (\ref{eq:3p21_final1}) and (\ref{eq:3p21_final2}) can be re-written as:
\begin{eqnarray} \label{eq:3p21_final}
\hat{B}(m) & = & \Sigma_X ^{-1/2} V ^{\star}_{(m)} V ^{\star T}_{(m)} \Sigma_X^{-1/2} \mathbf{X}^T \mathbf{Y}\\
& = & \Sigma_X^{-1}\mathbf{X}^T \mathbf{Y}\,\Sigma_Y^{-1/2} U ^{\star}_{(m)}U ^{\star T}_{(m)}  \Sigma_Y^{1/2}
\end{eqnarray}
The second equation makes the connection with the OLS coefficients $\hat{B}$:
\begin{eqnarray} \label{eq:3p21_e4}
\hat{B}(m) = \hat{B}\, \Sigma_Y^{-1/2} U ^{\star}_{(m)}U ^{\star T}_{(m)}  \Sigma_Y^{1/2}
\end{eqnarray}
Now:
\begin{itemize}
    \item For $\Sigma_Y = \mathbf{Y}^T \mathbf{Y}$, the matrix $U ^{\star}$ is precisely the one used in CCA, so $\Sigma_Y^{-1/2} U ^{\star}_{(m)}$ is the matrix $U_{(m)}$ of the top $m$ left CCA vectors and $\Sigma_Y^{1/2} U ^{\star}_{(m)}$ is its pseudo-inverse $U_{(m)} ^{-}$:
    \begin{eqnarray*}
        U_{(m)} ^{T} U_{(m)} ^{-} = \mathbb{I}_m
    \end{eqnarray*}
    \item Equation ($\ref{eq:3p21_e4}$) shows that $\hat{B}(m)$ depends on $\Sigma_Y$ both explicitly and implicitly via the dependence of $U ^{\star}$ through the generalized SVD decomposition (\ref{eq:3p21_e2}). Equation (\ref{eq:3p21_final}) shows that this dependence is fully encoded in the dependence of $V ^{\star}$ on $\Sigma_Y$. As a side note, notice that multiplying $\Sigma_Y$ by a constant does not change $U ^{\star}$ or $V ^{\star}$, but merely rescales $D ^{\star}$. Therefore, (\ref{eq:3p21_final}) shows that $\hat{B}(m)$ is invariant under such rescaling.
    
    We now show that $\hat{B}(m)$ has the same value for $\Sigma_Y = \Sigma_{0} \equiv \mathbf{Y}^T \mathbf{Y}$ and  $\Sigma_Y = \Sigma_{ols} \equiv \left( \mathbf{Y} - \mathbf{X} \hat{B} \right)^T \left( \mathbf{Y} - \mathbf{X} \hat{B} \right)$, by showing that $V ^{\star}$ is the same for these two choices. Let's fix the notation for the two SVD decompositions:
    \begin{eqnarray} \label{eq:3p21_svd1}
    (\mathbf{Y}^T \mathbf{X}) \Sigma_X^{-1/2} & = & \Sigma_{0}^{1/2} \;U_0 ^{\star} D_0 ^{\star}\; V_0 ^{\star T} \\
    \label{eq:3p21_svd2} & = & \Sigma_{ols}^{1/2} \;U_{ols} ^{\star}\; D_{ols} ^{\star}\; V_{ols} ^{\star T}
    \end{eqnarray}
    Using the expression of $\hat{B}$ and the first of these two SVD decompositions, one can easily derive:
    \begin{eqnarray} \label{eq:3p21_ols}
    \Sigma_{ols} = \Sigma_{0}^{1/2} \;\left( \mathbb{I} - U_0 ^{\star}\, D ^{\star 2}_0\, U_0 ^{\star T} \right) \Sigma_{0}^{1/2}
    \end{eqnarray}
    Now consider the matrix:
    \begin{eqnarray*}
        A = \Sigma_{ols}^{-1/2} \Sigma_{0}^{1/2} U ^{\star}_0
    \end{eqnarray*}
    Using (\ref{eq:3p21_ols}), one can see that:
    \begin{eqnarray*}
        A^TA & = & U_0 ^{\star T} \; \left( \mathbb{I}_K - U ^{\star}_0 D ^{\star 2}_0 U_0 ^{\star T} \right)^{-1} U_0 ^{\star} \\
        & = & \left( \mathbb{I}_q - D ^{\star 2}_0 \right)^{-1}
    \end{eqnarray*}
    Therefore, the matrix:
    \begin{eqnarray*}
        U ^{\star}_{cand}  =  A \, \left( \mathbb{I}_q - D ^{\star 2}_0 \right)^{1/2} = \Sigma_{ols}^{-1/2} \Sigma_{0}^{1/2} U ^{\star}_0 \left( \mathbb{I}_q - D ^{\star 2}_0 \right)^{1/2}
    \end{eqnarray*}
    is orthogonal and:
    \begin{eqnarray*}
        (\mathbf{Y}^T \mathbf{X}) \Sigma_X^{-1/2} & = & \Sigma_{0}^{1/2} \;U_0 ^{\star} D_0 ^{\star}\; V_0 ^{\star T}\\
        & = & \Sigma_{ols} ^{1/2} U_{cand} ^{\star} \left( \mathbb{I}_q - D ^{\star 2}_0 \right)^{-1/2} D_0 ^{\star}\; V_0 ^{\star T}
    \end{eqnarray*}
    Comparing this expression with (\ref{eq:3p21_svd2}), we conclude:
    \begin{eqnarray*}
        U ^{\star}_{ols} & = & U ^{\star}_{cand} \\
        D ^{\star}_{ols} & = & \left( \mathbb{I}_q - D ^{\star 2}_0 \right)^{-1/2} D_0 ^{\star} \\
        V ^{\star}_{ols} & = & V ^{\star}_0
    \end{eqnarray*}
    Equation (\ref{eq:3p21_final}) lets us immediately deduce $\hat{B}_{ols}(m) = \hat{B}_0(m)$.
\end{itemize}
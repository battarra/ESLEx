%!TEX root = ../Tibt.tex

\exercise{3.26}

Denote $\mathbf{x}_{j, \mathcal{A}}$ the residual of $\mathbf{x}_j$
against the active predictors $\mathbf{X}_\mathcal{A}$:
\begin{eqnarray*}
    \mathbf{x}_j & = & \mathbf{x}_{j, \mathcal{A}} + \mathbf{X}_\mathcal{A} \gamma_j \\
    \mathbf{X}_\mathcal{A}^T \mathbf{x}_{j, \mathcal{A}} & = & 0
\end{eqnarray*}
Using the last relationship, the RSS with $j$ included simplifies
to:
\begin{eqnarray*}
    || \mathbf{y} - \mathbf{X}_\mathcal{A} \beta_\mathcal{A} - 
    \mathbf{x}_j \beta_j||^2 & = & || \mathbf{y} - \mathbf{X}_\mathcal{A} \beta_\mathcal{A}'||^2 + || \mathbf{y} - 
    \mathbf{x}_{j, \mathcal{A}} \beta_j||^2 - ||\mathbf{y}||^2  \\
    \beta_\mathcal{A}' & \equiv & \beta_\mathcal{A} + \beta_j \gamma_j
\end{eqnarray*}
This shows that the OLS minimization can be carried over on $\beta'_\mathcal{A}$ 
and $\beta_j$ independently. Therefore, the OLS coefficient $\hat{\beta}_j$
is the single variable regression coefficient of $\mathbf{y}$ against 
$\mathbf{x}_{j, \mathcal{A}}$, which had already been shown in Section 3.2.3
of the text. At the OLS value:
\begin{equation*}
|| \mathbf{y} - \mathbf{x}_{j, \mathcal{A}} \hat{\beta_j}||^2 - ||\mathbf{y}||^2 =
||\mathbf{y}||^2 \left(1 - \rho_{j, \mathcal{A}}^2\right)
\end{equation*}
where $\rho_{j, \mathcal{A}}$ is the correlation between $\mathbf{y}$ and
$\mathbf{x}_{j, \mathcal{A}}$. Therefore, in forward stepwise regression,
the largest decrease in RSS is provided by the predictor whose residual w.r.t. the 
active predictors is mostly correlated to $\mathbf{y}$ in absolute value,
as pointed out by the hint.

In a way, LAR residualises the response against the active predictors 
before computing correlations, while forward stepwise residualises the candidate
predictors themselves. These two operations result in potentially very different
correlations. Denoting $\mathbf{P}_\mathcal{A} \equiv \mathbf{X}_\mathbf{A} 
\left(\mathbf{X}_\mathbf{A}^T \mathbf{X}_\mathbf{A}\right)^{-1} 
\mathbf{X}_\mathcal{A}$ the projector onto the subspace generated by the
columns of $\mathbf{X}_\mathcal{A}$, one has:
\begin{eqnarray*}
\textrm{LAR}: & & \textrm{Corr} \left(\mathbf{x}_j, \mathbf{y}_{\mathcal{A}}\right) =
    \frac{\mathbf{x}_j^T P_\mathcal{A} \mathbf{y}}{
        ||\mathbf{x}_j||\,||\mathbf{y}_{\mathcal{A}}||} = 
    \frac{\mathbf{y}^T P_\mathcal{A}\mathbf{x}_j}{
        ||\mathbf{x}_j||\,||\mathbf{y}_{\mathcal{A}}||} = 
    \frac{\mathbf{y}^T \mathbf{x}_{j, \mathcal{A}}}{
        ||\mathbf{x}_j||\,||\mathbf{y}_{\mathcal{A}}||} \\
\textrm{Forward stepwise}: & &
    \textrm{Corr} \left(\mathbf{x}_{j, \mathcal{A}}, \mathbf{y}\right) = 
        \frac{\mathbf{y}^T \mathbf{x}_{j, \mathcal{A}}}{
            ||\mathbf{x}_{j, \mathcal{A}}||\,||\mathbf{y}||}
\end{eqnarray*}
While the numerator is the same, the denominator is different between
the two expressions. In particular, the denominator in the second expression
tends to more aggressively 'squeeze' out of all predictors their idiosyncratic
part that is correlated to the response. Consider the following example:
\begin{eqnarray*}
    \mathbf{y} & = & \mathbf{X}_\mathcal{A} \beta_\mathcal{A} + \eta\, \mathbf{v} + \bm{\epsilon}_1 \\
    \mathbf{x}_1 & = & \mathbf{X}_\mathcal{A} \gamma_\mathcal{A} + \mathbf{v} + \bm{\epsilon}_2 \\
    \mathbf{x}_2 & = & \mathbf{v} + \sqrt{1 + ||\mathbf{X}_\mathcal{A} \gamma_\mathcal{A}||^2}\,\bm{\epsilon}_3 \\
    \bm{\epsilon}_i^T \mathbf{X}_\mathcal{A} & = & \bm{\epsilon}_i^T \mathbf{v} = 0\\
    ||\bm{\epsilon}_1||^2 & = & ||\bm{\epsilon}_2||^2
\end{eqnarray*}
The two predictors have the same norm and their residuals have the same
covariance with $\mathbf{y}$:
\begin{eqnarray*}
    \mathbf{x}_{1, \mathcal{A}} & = & \mathbf{v} + \bm{\epsilon}_1 \\
    \mathbf{x}_{2, \mathcal{A}} & = & \mathbf{x}_{2} = \mathbf{v} + \sqrt{1 + ||\mathbf{X}_\mathcal{A} \gamma_\mathcal{A}||^2}\,\bm{\epsilon}_2
\end{eqnarray*}
hence they behave similarly w.r.t. LAR. On the other hand:
\begin{equation*}
||\mathbf{x}_{1, \mathcal{A}}||^2 = ||\mathbf{v}||^2 + ||\bm{\epsilon}_1||^2 < ||\mathbf{x}_{2, \mathcal{A}}||^2 = ||\mathbf{v}||^2 + \left(1 + ||\mathbf{X}_\mathcal{A} \gamma_\mathcal{A}||^2\right) ||\bm{\epsilon}_2||^2
\end{equation*}
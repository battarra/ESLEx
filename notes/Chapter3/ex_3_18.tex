%!TEX root = ../Tibt.tex

\exercise{3.18}

\subsection*{Review of PLS}
Let's review the PLS algorithm:

\begin{eqnarray}
    \mathbf{z}_m & = & \mathbf{X}^{(m - 1)} \mathbf{X}^{(m - 1)\, T} \mathbf{y} \\ \label{eq:3p18_recy}
    \hat{\mathbf{y}}_m & = & \hat{\mathbf{y}}_{m - 1} + \frac{\mathbf{z}_m \, \mathbf{z}_m^T}{||\mathbf{z}_m||^2}\, \mathbf{y}\\
    \\
    \mathbf{X}^{(m)} & = & \mathbf{X}^{(m - 1)} - \frac{\mathbf{z}_m \, \mathbf{z}_m^T}{||\mathbf{z}_m||^2} \mathbf{X}^{(m - 1)}
\end{eqnarray}
Denoting $P_m$ the euclidean projector over the subspace generated by $\mathbf{z}_m$:
\begin{equation*}
P_m  \equiv \frac{\mathbf{z}_m \, \mathbf{z}_m^T}{||\mathbf{z}_m||^2}: \quad P_m^2 = P_m, \;\; P_m \, \mathbf{z}_m = \mathbf{z}_m
\end{equation*}
One has then:
\begin{eqnarray}
    \hat{\mathbf{y}}_m & = & \left(P_1 + \ldots P_m\right) \mathbf{y}  \\ 
    \label{eq:3p18_recX}
    \mathbf{X}^{(m)} & = & \left(1 - P_m\right) \mathbf{X}^{(m - 1)}    
\end{eqnarray}

$\mathbf{X}^{(m)}$ is orthogonal not only to $\mathbf{z}_m$, but also to
all previous $\mathbf{z}$'s:
\begin{equation*}
\mathbf{X}^{(m)\,T} \mathbf{z}_n \;\; \forall\; n \leq m
\end{equation*}
We can prove this by recursion over $m$. Observe that:
\begin{eqnarray*}
    \mathbf{X}^{(m)\,T} \mathbf{z}_n & = & \mathbf{X}^{(m - 1)\,T}
        \left(1 - P_m\right) \mathbf{z}_n
\end{eqnarray*}
For $n = m$ the rhs vanishes because $P_m \, \mathbf{z}_m = \mathbf{z}_m$,
while for $n \leq m - 1$ one has:
\begin{equation*}
P_m\, \mathbf{z_n} = \frac{\mathbf{z}_m}{||\mathbf{z}_m||^2} \mathbf{z}_m^T \mathbf{z}_n = \frac{\mathbf{z}_m}{||\mathbf{z}_m||^2} \mathbf{y}^T \mathbf{X}^{(m - 1)} \mathbf{X}^{(m - 1)\, T} \mathbf{z}_n = 0
\end{equation*}
by recursive hypothesis on the product of the last two terms.

Since $\mathbf{z}_{m + 1}$ is a linear combination of the columns of
$\mathbf{X}^{(m)}$, this also implies that the $\mathbf{z}$'s are mutually
orthogonal:
\begin{eqnarray*}
    \mathbf{z}_m^T \, \mathbf{z}_n & = & 0 \;\; \forall\; n \neq m\\
    P_m \, P_n & = & 0  \;\; \forall\; n \neq m    
\end{eqnarray*}
The recurrence in Eq. \ref{eq:3p18_recX} can therefore be 'unrolled' as:

\begin{equation}
\mathbf{X}^{(m)} = \left(1 - P_1 - \ldots - P_m\right) \mathbf{X}
\end{equation}

\subsection*{Connection with CGA: notation}
The connection with conjugate gradients algorithms (CGA) is at the level
of the regression coefficients $\beta$. As explained in the book,
all $\mathbf{X}^{(m)}$'s are linear combinations of the original $\mathbf{X}$, therefore:
\begin{equation}
\hat{\mathbf{y}}_m = \mathbf{X}\beta_m
\end{equation}
for some vector $\beta_m$. If we compare with Eq. \ref{eq:3p18_recy},
we see that:
\begin{equation*}
\hat{\mathbf{y}}_m - \hat{\mathbf{y}}_{m - 1} = 
    \mathbf{X} \left(\beta_m - \beta_{m - 1}\right) \propto
    \mathbf{z}_m
\end{equation*}
In CGA, the value of a function $L(\beta)$ is minimized by
successively moving $\beta$ along some directions $p_m$:
\begin{equation*}
\beta_m - \beta_{m - 1} \propto p_{m - 1}
\end{equation*}
Therefore, in order to establish a connection, we denote:
\begin{equation}\label{eq:e3p18_p}
\mathbf{X} p_{m - 1} \equiv \mathbf{z}_m
\end{equation}
Notice that this equation determines $p_{m - 1}$ uniquely
as long as the $\mathbf{X}$ are linearly independent.

Finally, we denote $l(\beta)$ the L2 loss and $g(\beta)$ its gradient:
\begin{eqnarray*}
    l(\beta) & \equiv & ||\mathbf{y} - \mathbf{X} \beta ||^2 \\
    g(\beta) & = & \nabla l(\beta) = - \mathbf{X}^T \left(\mathbf{y}
        - \mathbf{X} \beta \right)
\end{eqnarray*}

\subsection*{Review of CGA}

The CGA algorithm tries to minimize a quadratic form:
\begin{equation*}
f(\beta) = c^T \beta + \frac{1}{2} x^T G x, \;\;x \in \mathbb{R}^p\;\; G = G^T
\end{equation*}
by recursively finding the optimal update of $x$ along
increasingly larger subspaces of $\mathbb{R}^p$. In practice,
one starts from a set of vectors:
$$
p_0, p_1, \ldots \in \mathbb{R}^p
$$
and recursively defines:
\begin{eqnarray*}
    x_{m + 1} & = & x_{m} + \,
    \textrm{argmin}_{w \in \mathcal{P}_{m}} f(x_{m} + w) \\
    \mathcal{P}_{m} & \equiv & \textrm{Span} \left(p_0, \ldots, p_m\right)    
\end{eqnarray*}
In particular, in conjugate gradient methods one takes:
\begin{eqnarray}
    \mathcal{P}_{m} & = & \textrm{Span} \left(g(x_0), \ldots, g(x_m)\right) \\
    p_0 & = & - g(x_0) \\ \label{eq:3p18_3}
    p_m & = & - g(x_m) + \gamma_{m - 1}\, p_{m - 1}
\end{eqnarray}
where $g(x)$ is the gradient of $f$, in accordance with
the notation of the previous section, and the coefficients $\gamma$ are chosen
in such a way that the $p_m$ are all mutually \textit{conjugate}:
\begin{equation*}
p_m^T G\,p_n = 0 \quad \forall \; m \neq n
\end{equation*}
The fact that the second term in Eq. \ref{eq:3p18_3} is 
sufficient to guarantee orthogonality is not trivial,
see \cite{Murray1982}, Section 4.8.3. Thanks to conjugacy,
the update $x_m \rightarrow x_{m + 1}$ consists in moving $x$
along the $p_m$ direction only:

\begin{equation}\label{eq:e3p18_update}
x_{m + 1} = x_m + \left(\textrm{argmin}_\alpha 
    f(x_m + \alpha \, p_{m})\right)\, p_m
\end{equation}

\subsection*{Connection between PLS and CGA}

We now show that PLS is nothing but CGA on the regression coefficients
applied to the L2 loss as a function of the regression coefficients
$\beta$, with initial condition $\beta_0 = 0$:
\begin{eqnarray*}
    f(\beta) & = & c^T \beta + \frac{1}{2} \beta^T G \beta \\
    c^T & = & - \mathbf{y}^T \mathbf{X} \\
    G & = & \mathbf{X}^T \mathbf{X}
\end{eqnarray*}
The update directions are specified by Eq. \ref{eq:e3p18_p}:
\begin{equation*}
\mathbf{X} p_m  =  \mathbf{z}_{m + 1} = \mathbf{X}_m \mathbf{X}_m^T \mathbf{y}
\end{equation*}
and they are indeed mutually conjugate:
\begin{equation}
p_m \, G \, p_n = p_m \mathbf{X}^T \mathbf{X} \, p_n = 
    \mathbf{z}_{m + 1} ^T \mathbf{z}_{n + 1} = 0, \;\; m \neq n
\end{equation}
Moreover, the coefficient of the $\mathbf{z}_m$ term at iteration $m$ of PLS
is the result of a least squares regression of $\mathbf{y}$ against
$\mathbf{z}_m$ itself, which is the equivalent of Eq. \ref{eq:e3p18_update}.

To complete the connection, we show that 
$$p_m = -g(\beta_m) + p_{m, \parallel}$$
where $p_{m, \parallel} \in \textrm{Span} \left(p_0, \ldots, p_{m - 1}\right)$,
so that indeed $\textrm{Span} \left(p_0, \ldots, p_m\right) = \textrm{Span} \left(g_0, \ldots, g_m\right)$. We note that:
\begin{eqnarray}
    - \mathbf{X} g(\beta_m) & = & \mathbf{X} \mathbf{X}^T \left(y - \mathbf{X} \beta_m\right) \\
    & = &  \mathbf{X} \mathbf{X}^T \left(1 - P_1 - \ldots - P_m\right) \mathbf{y} \\
    & = & \mathbf{X} \left((1 - P_1- \ldots P_m) \mathbf{X}\right)^T \mathbf{y} \\
    \label{eq:e3p18_passage}
    & = & \mathbf{X} \mathbf{X}_m^T\, \mathbf{y}
\end{eqnarray}
Remember that:
\begin{equation}
\mathbf{X}_m = \mathbf{X} 
    - \mathbf{z}_1 \frac{\mathbf{z}_1^T \mathbf{X}}{||\mathbf{z}_1||^2} - \ldots
    - \mathbf{z}_m \frac{\mathbf{z}_m^T \mathbf{X}}{||\mathbf{z}_m||^2}
\end{equation}
Plugging in the previous equation in Eq. \ref{eq:e3p18_passage} we obtain:
\begin{eqnarray*}
    - \mathbf{X} g(\beta_m) & = & \mathbf{X}_m \mathbf{X}_m^T\, \mathbf{y} + 
        c_1\, \mathbf{z_1} + \ldots + c_m\, \mathbf{z}_m \\
        & = & \mathbf{z}_{m + 1} + c_1\, \mathbf{z_1} + \ldots + c_m\, \mathbf{z}_m
\end{eqnarray*}
or, in other terms:
\begin{equation*}
\mathbf{X} p_m = - \mathbf{X}\, g(\beta_m) + \mathbf{X} p_{m, \parallel}
\end{equation*}
which completes our proof.
